{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 심층 신경망 훈련\n",
    "- 수 많은 은닉층과 뉴런을 가진 심층 신경망을 훈련시킬 때 직면하는 문제들\n",
    "    - 1. 까다로운 그래디언트 소실(또는 폭주) 문제에 직면할 것(이 현상은 심층 신경망에 영향을 주며 하위층이 훈련하기 매우 어려워짐)\n",
    "    - 2. 이런 대규모 신경망에서는 훈련이 극단적으로 느려질 것\n",
    "    - 3. 수백만 개의 파라미터를 가진 모델은 훈련 세트에 과대적합될 위험이 큼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 맷플롯립 설정\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# 일관된 출력을 위해 유사난수 초기화\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "# 한글출력\n",
    "plt.rcParams['font.family'] = 'NanumBarunGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 그래디언트 소실과 폭주 문제\n",
    "- 그래디언트 소실은 알고리즘이 하위층으로 진행됨에 따라 그래디언트가 점점작아져 하위층의 연결 가중치는 실제 변경되지 않는 것(주로 심층신경망)\n",
    "- 그래디언트 폭주는 그래디언트가 점점 켜져 여러 개의 층이 비정상적으로 큰 가중치로 갱신되며 알고리즘이 발산하는 것(주로 순환신경망)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('convergence', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\") # 수렴\n",
    "plt.annotate('convergence', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\") # 수렴\n",
    "plt.annotate('linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\") # 선형\n",
    "plt.grid(True)\n",
    "plt.title(\"Logistic activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "plt.show() # 로지스틱 함수는 수렴(convergence)인 경우 기울기가 0에 가까워짐 -> 하위층에서는 그래디언트가 0에 가까워 전달이 안되는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.1.1 세이비어 초기화와 He 초기화\n",
    "- 이런 그래디언트 문제의 해결책으로, 예측을 할 때는 정방향으로, 그래디언트를 역전파할 때는 역뱡항으로 양방향 신호가 적절히 흘러야 한다고 주장\n",
    "    - 적절한 신호가 흐르기 위해 각 층의 출력에 대한 분산이 입력에 대한 분산과 같아야 함\n",
    "    - 역방향에서 층을 통과하기 전과 후의 그래디언트 분산이 동일해야 함\n",
    "- 층의 입력과 출력 연결 개수가 같지 않다면 이 두 가지를 보장할 수 없음\n",
    "    - 이를 해결하기 위해 무작위 초기화하는 것을 선택(이를 세이비어 초기화, 글로럿 초기화라고 함)\n",
    "    - 10장에서 사용한 tf.layer.dence는 기본적으로 세이비어 초기화를 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yoo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Yoo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Yoo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Yoo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Yoo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Yoo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-ad191c782d81>:3: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Yoo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "he_init = tf.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                          kernel_initializer=he_init, name=\"hidden1\") # 세이비어가 아닌 He 초기화 방식(he_init)으로 바꿀 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.1.2 수렴하지 않는 활성화 함수\n",
    "- 활성화 함수를 잘못 선택하면 자칫 그래디언트 소실이나 폭주의 문제로 이어질 수 있음\n",
    "- ReLU의 경우 양숫값에서 수렴하지 않지만, 완벽하지는 않음\n",
    "    - 음수가 되면 그래디언트가 0이 되기 때문에 죽은 ReLU로 알려진 훈련하는 동안 일부 뉴런이 0 이외의 다른 값을 출력하지 않는 문제가 발생함\n",
    "- 이런 문제를 해결하기 위해 LeakyReLU 같은 ReLU 변종 함수를 사용\n",
    "    - ReLU는 음수일 경우 뉴런이 계속 0을 출력하여 살아나기 힘들었지만, LeakyReLU는 일반적으로 0.01의 기울기를 가져 다시 깨어날 가능성을 가짐\n",
    "- 또 다른 변종 함수 ELU는 다른 모든 시험에서 ReLU를 앞선 성능을 보여줌(다만, 속도는 ReLU보다 느림)\n",
    "    - 1. z < 0일 때 음수값이 들어오므로 활성화 함수의 평균 출력이 0에 더 가까워짐(이는 그래디언트 소실 문제를 완화해줌)\n",
    "    - 2. z < 0이어도 그래디언트가 0이 아니므로 죽은 뉴런을 만들지 않음\n",
    "    - 3. 하이퍼파리미터 알파 = 1일 때 이 함수는 z = 0에서 급격히 변동하지 않고 z = 0을 포함해 모든 구간에서 매끄러워 경사 하강법 속도를 높임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Pass', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\") # 통과\n",
    "plt.title(\"Leaky ReLU\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 텐서플로를 통한 LeakyReLU 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\") # 활성화 함수 LeakyReLU 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 정의\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"): # 은닉층 설정\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")  # 활성화 함수 leaky_relu 함수\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\") # 활성화 함수 leaky_relu 함수\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"): # 비용함수 설정\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"): # 역전파 훈련 설정\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"): # 평가 설정\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer() # 초기화\n",
    "saver = tf.train.Saver() # 훈련 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 셔플 함수\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 배치 데이터 정확도: 0.86 검증 세트 정확도: 0.9044\n",
      "5 배치 데이터 정확도: 0.94 검증 세트 정확도: 0.9494\n",
      "10 배치 데이터 정확도: 0.92 검증 세트 정확도: 0.9656\n",
      "15 배치 데이터 정확도: 0.94 검증 세트 정확도: 0.9714\n",
      "20 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.9764\n",
      "25 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.9776\n",
      "30 배치 데이터 정확도: 0.98 검증 세트 정확도: 0.978\n",
      "35 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.9788\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "            print(epoch, \"배치 데이터 정확도:\", acc_batch, \"검증 세트 정확도:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 텐서플로를 통한 ELU 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")  # 활성화 함수를 tf.nn.elu 설정하여 텐서플로에서 ELU 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SELU\n",
    "- 훈련할 때 SELU 활성화 함수를 사용한 완전 연결 신경망은 스스로 정규화\n",
    "- 각 층의 출력은 훈련하는 동안 같은 평균과 분산을 유지하려는 경향이 있어 그래디언트 소실과 폭주 문제를 해결\n",
    "- 이 활성화 함수는 심층 신경망에서 다른 활성화 함수보다 뛰어난 성능을 내므로 꼭 이 함수를 시도해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"SELU\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기본적으로 SELU 하이퍼파라미터(scale과 alpha)는 평균이 0, 표준 편차가 1에 가깝게 유지되도록 조정(입력도 평균이 0, 표준 편차가 1로 표준화되었다고 가정)\n",
    "- 이 활성화 함수를 사용하면 100층으로 된 심층 신경망도 그래디언트 소실/폭주 문제없이 모든 층에서 대략 평균이 0이고 표준 편차가 1을 유지\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "층 0: -0.26 < 평균 < 0.27, 0.74 < 표준 편차 < 1.27\n",
      "층 10: -0.24 < 평균 < 0.27, 0.74 < 표준 편차 < 1.27\n",
      "층 20: -0.17 < 평균 < 0.18, 0.74 < 표준 편차 < 1.24\n",
      "층 30: -0.27 < 평균 < 0.24, 0.78 < 표준 편차 < 1.20\n",
      "층 40: -0.38 < 평균 < 0.39, 0.74 < 표준 편차 < 1.25\n",
      "층 50: -0.27 < 평균 < 0.31, 0.73 < 표준 편차 < 1.27\n",
      "층 60: -0.26 < 평균 < 0.43, 0.74 < 표준 편차 < 1.35\n",
      "층 70: -0.19 < 평균 < 0.21, 0.75 < 표준 편차 < 1.21\n",
      "층 80: -0.18 < 평균 < 0.16, 0.72 < 표준 편차 < 1.19\n",
      "층 90: -0.19 < 평균 < 0.16, 0.75 < 표준 편차 < 1.20\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100))\n",
    "for layer in range(100):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1/100))\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=1)\n",
    "    stds = np.std(Z, axis=1)\n",
    "    if layer % 10 == 0:\n",
    "        print(\"층 {}: {:.2f} < 평균 < {:.2f}, {:.2f} < 표준 편차 < {:.2f}\".format(\n",
    "            layer, means.min(), means.max(), stds.min(), stds.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 텐서플로를 통한 SELU 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"): # 은닉층 설계\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\") # 활성화 함수 selu 함수\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\") # 활성화 함수 selu 함수\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"): # 비용함수 설정\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"): # 역전파 훈련 설정\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"): # 평가 설정\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 배치 데이터 정확도: 0.88 검증 세트 정확도: 0.9232\n",
      "5 배치 데이터 정확도: 0.98 검증 세트 정확도: 0.9576\n",
      "10 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.9666\n",
      "15 배치 데이터 정확도: 0.96 검증 세트 정확도: 0.9684\n",
      "20 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.9692\n",
      "25 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.969\n",
      "30 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.9692\n",
      "35 배치 데이터 정확도: 1.0 검증 세트 정확도: 0.9698\n"
     ]
    }
   ],
   "source": [
    "means = X_train.mean(axis=0, keepdims=True)\n",
    "stds = X_train.std(axis=0, keepdims=True) + 1e-10\n",
    "X_val_scaled = (X_valid - means) / stds\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_val_scaled, y: y_valid})\n",
    "            print(epoch, \"배치 데이터 정확도:\", acc_batch, \"검증 세트 정확도:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final_selu.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.1.3 배치 정규화\n",
    "- ReLU의 변종 함수를 사용하면 소실이나 폭주 문제를 크게 감소시킬 수 있지만, 훈련하는 동안 다시 발생하지 않으리란 보장은 없음\n",
    "- 이를 위해서 배치 정규화 기법을 사용\n",
    "    - 이 기법은 각 층에서 활성화 함수를 통과하기 전에 모델에 연산을 하나 추가함\n",
    "    - 단순하게 입력 데이터의 평균을 0으로 만들고 정규화함(이를 위해서는 평균과 표준편차를 추정해야 함)\n",
    "    - 각 층에서 두 개의 새로운 파라미터 결과값의 스케일을 조정하고 이동시킴\n",
    "    - 두 개의 결과값은 하나는 스케일 조정을 위해, 다른 하나는 이동을 위해 필요함\n",
    "    - 즉, 이 연산으로 모델이 층마다 입력 데이터의 최적 스케일과 평균을 학습\n",
    "- 이 방법은 대부분 성능을 향상시킬 것이지만, 모델의 복잡도를 키워 시간이 오래 걸림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 텐서플로를 통한 배치 정규화 구현\n",
    "- tf.nn.batch.normalization() 함수를 통해 입력값을 중앙에 정렬 및 정규화가 가능하지만, 이후 과정이 복잡함\n",
    "- tf.layers.batch_normalization() 함수는 일련의 과정을 모두 처리해주므로 이 함수를 사용하는 것이 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-24-e9da75027a24>:13: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training') # 훈련 동안에는 True, 이외에는 False\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\") # 완전 연결 층(활성화 함수 지정 X -> 정규화 층 이후 활성화 함수 지정하므로)\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9) # 정규화 층\n",
    "bn1_act = tf.nn.elu(bn1) # 활성화 함수 층?\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\") # 완전 연결 층\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9) # 정규화 층\n",
    "bn2_act = tf.nn.elu(bn2) # 활상화 함수 층?\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\") # 완전 연결 층\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=0.9) # 정규화 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial # partial ==> 같은 매개변수를 반복적으로 사용하지 않기 위한 모듈\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9) # 나만의 기준을 적용\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1) # my_batch_norm_layer 사용\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2) # my_batch_norm_layer 사용\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn) # my_batch_norm_layer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Yoo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=batch_norm_momentum) # 정규화 층 기준\n",
    "\n",
    "    my_dense_layer = partial(tf.layers.dense, kernel_initializer=he_init) # 완전 연결 층 기준\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    \n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    \n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기존 실행 단계와 다른 두 가지\n",
    "    - 1. 훈련하는 동안에는 batch_normalization() 함수에 의존하는 어떤 연산을 수행할 때마다 training 플레이스홀더를 True 설정\n",
    "    - 2. batch_normalization() 함수는 이동 평균을 갱신하기 위해 매 훈련 단계에서 평가할 몇 개의 연산을 생성\n",
    "        - 이 연산은 자동으로 UPDATE_OPS 컬렉션에 추가되므로 컬렉션에서 이 연산능 뽑아내어 훈련이 반복될 때마다 실행해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 검증 세트 정확도: 0.8952\n",
      "1 검증 세트 정확도: 0.9202\n",
      "2 검증 세트 정확도: 0.9318\n",
      "3 검증 세트 정확도: 0.9422\n",
      "4 검증 세트 정확도: 0.9468\n",
      "5 검증 세트 정확도: 0.954\n",
      "6 검증 세트 정확도: 0.9568\n",
      "7 검증 세트 정확도: 0.96\n",
      "8 검증 세트 정확도: 0.962\n",
      "9 검증 세트 정확도: 0.9638\n",
      "10 검증 세트 정확도: 0.9662\n",
      "11 검증 세트 정확도: 0.9682\n",
      "12 검증 세트 정확도: 0.9672\n",
      "13 검증 세트 정확도: 0.9696\n",
      "14 검증 세트 정확도: 0.9706\n",
      "15 검증 세트 정확도: 0.9704\n",
      "16 검증 세트 정확도: 0.9718\n",
      "17 검증 세트 정확도: 0.9726\n",
      "18 검증 세트 정확도: 0.9738\n",
      "19 검증 세트 정확도: 0.9742\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # UPDATE_OPS에서 연산을 뽑아냄\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X: X_batch, y: y_batch}) # extra_update_ops 들어감\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\") # 층이 두 개뿐인 간단한 예제가 아닌 심층 신경망에서는 큰 차이를 만들 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변형\n",
    "\"\"\"\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\"\"\"\n",
    "# 만약 훈련 세트에서 UPDATE_OPS를 적용하면\n",
    "\"\"\"\n",
    "sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "\"\"\"\n",
    "# 실행 단계에서 이렇게 시행할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 추가\n",
    "- 훈련될 변수 개수가 전체 전역 변수 개수보다 적은데, 이는 이동 평균을 위한 변수는 훈련되는 변수가 아니기 때문\n",
    "- 미리 학습한 신경망을 재사용할 경우(아래 참조) 이런 훈련되지 않는 변수를 놓쳐서는 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.trainable_variables()] # 훈련될 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/moving_mean:0',\n",
       " 'batch_normalization/moving_variance:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/moving_mean:0',\n",
       " 'batch_normalization_1/moving_variance:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/moving_mean:0',\n",
       " 'batch_normalization_2/moving_variance:0']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.global_variables()] # 전역 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.1.4 그래디언트 클리핑\n",
    "- 그래디언트 폭주 문제를 줄이는 쉬운 방법은 역전파될 때 일정 임곗값을 넘어서지 못하게 그래디언트를 단순히 잘라내는 것\n",
    "    - 순환 신경망에서는 일반적으로 널리 사용하지만 다른 경우에는 대부분 배치 정규화를 선호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"): # 5개의 은닉층을 가짐\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss) # compute_gradients() 사용\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars] # clip_by_value() ==> 그래디언트 클리핑하는 연산을 생성\n",
    "training_op = optimizer.apply_gradients(capped_gvs) # apply_gradients ==> 클리핑된 그래디언트를 적용하는 연산을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 검증 세트 정확도: 0.2878\n",
      "1 검증 세트 정확도: 0.7942\n",
      "2 검증 세트 정확도: 0.8796\n",
      "3 검증 세트 정확도: 0.9064\n",
      "4 검증 세트 정확도: 0.9162\n",
      "5 검증 세트 정확도: 0.922\n",
      "6 검증 세트 정확도: 0.929\n",
      "7 검증 세트 정확도: 0.9356\n",
      "8 검증 세트 정확도: 0.9382\n",
      "9 검증 세트 정확도: 0.9418\n",
      "10 검증 세트 정확도: 0.9458\n",
      "11 검증 세트 정확도: 0.9472\n",
      "12 검증 세트 정확도: 0.9476\n",
      "13 검증 세트 정확도: 0.9536\n",
      "14 검증 세트 정확도: 0.9568\n",
      "15 검증 세트 정확도: 0.9566\n",
      "16 검증 세트 정확도: 0.9576\n",
      "17 검증 세트 정확도: 0.9592\n",
      "18 검증 세트 정확도: 0.9622\n",
      "19 검증 세트 정확도: 0.9612\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 미리 훈련된 층 재사용하기\n",
    "- 일반적으로 아주 큰 규모의 DNN을 처음부터 새로 훈련시키는 것은 좋은 방법이 아님\n",
    "- 해결하려는 것과 비슷한 유형의 문제를 처리한 신경망이 있는지 찾아본 후, 그런 신경망의 하위층을 재사용하는 것이 좋음(전이 학습)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.1 텐서플로 모델 재사용하기\n",
    "- import_meta_graph() 함수가 그래프 연산들을 로드하여 기본 그래프에 적재하고 모델의 상태를 복원할 수 있도록 Saver 객체를 반환\n",
    "- 기본적으로 Saver 객체는 .meta 확장자를 가진 파일에 그래프 구조를 저장하므로 이 파일을 로드해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\") # import_meta_graph()를 통해 그래프 연산을 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "y\n",
      "hidden1/kernel/Initializer/random_uniform/shape\n",
      "hidden1/kernel/Initializer/random_uniform/min\n",
      "hidden1/kernel/Initializer/random_uniform/max\n",
      "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden1/kernel/Initializer/random_uniform/sub\n",
      "hidden1/kernel/Initializer/random_uniform/mul\n",
      "hidden1/kernel/Initializer/random_uniform\n",
      "hidden1/kernel\n",
      "hidden1/kernel/Assign\n",
      "hidden1/kernel/read\n",
      "hidden1/bias/Initializer/zeros\n",
      "hidden1/bias\n",
      "hidden1/bias/Assign\n",
      "hidden1/bias/read\n",
      "dnn/hidden1/MatMul\n",
      "dnn/hidden1/BiasAdd\n",
      "dnn/hidden1/Relu\n",
      "hidden2/kernel/Initializer/random_uniform/shape\n",
      "hidden2/kernel/Initializer/random_uniform/min\n",
      "hidden2/kernel/Initializer/random_uniform/max\n",
      "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden2/kernel/Initializer/random_uniform/sub\n",
      "hidden2/kernel/Initializer/random_uniform/mul\n",
      "hidden2/kernel/Initializer/random_uniform\n",
      "hidden2/kernel\n",
      "hidden2/kernel/Assign\n",
      "hidden2/kernel/read\n",
      "hidden2/bias/Initializer/zeros\n",
      "hidden2/bias\n",
      "hidden2/bias/Assign\n",
      "hidden2/bias/read\n",
      "dnn/hidden2/MatMul\n",
      "dnn/hidden2/BiasAdd\n",
      "dnn/hidden2/Relu\n",
      "hidden3/kernel/Initializer/random_uniform/shape\n",
      "hidden3/kernel/Initializer/random_uniform/min\n",
      "hidden3/kernel/Initializer/random_uniform/max\n",
      "hidden3/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden3/kernel/Initializer/random_uniform/sub\n",
      "hidden3/kernel/Initializer/random_uniform/mul\n",
      "hidden3/kernel/Initializer/random_uniform\n",
      "hidden3/kernel\n",
      "hidden3/kernel/Assign\n",
      "hidden3/kernel/read\n",
      "hidden3/bias/Initializer/zeros\n",
      "hidden3/bias\n",
      "hidden3/bias/Assign\n",
      "hidden3/bias/read\n",
      "dnn/hidden3/MatMul\n",
      "dnn/hidden3/BiasAdd\n",
      "dnn/hidden3/Relu\n",
      "hidden4/kernel/Initializer/random_uniform/shape\n",
      "hidden4/kernel/Initializer/random_uniform/min\n",
      "hidden4/kernel/Initializer/random_uniform/max\n",
      "hidden4/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden4/kernel/Initializer/random_uniform/sub\n",
      "hidden4/kernel/Initializer/random_uniform/mul\n",
      "hidden4/kernel/Initializer/random_uniform\n",
      "hidden4/kernel\n",
      "hidden4/kernel/Assign\n",
      "hidden4/kernel/read\n",
      "hidden4/bias/Initializer/zeros\n",
      "hidden4/bias\n",
      "hidden4/bias/Assign\n",
      "hidden4/bias/read\n",
      "dnn/hidden4/MatMul\n",
      "dnn/hidden4/BiasAdd\n",
      "dnn/hidden4/Relu\n",
      "hidden5/kernel/Initializer/random_uniform/shape\n",
      "hidden5/kernel/Initializer/random_uniform/min\n",
      "hidden5/kernel/Initializer/random_uniform/max\n",
      "hidden5/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden5/kernel/Initializer/random_uniform/sub\n",
      "hidden5/kernel/Initializer/random_uniform/mul\n",
      "hidden5/kernel/Initializer/random_uniform\n",
      "hidden5/kernel\n",
      "hidden5/kernel/Assign\n",
      "hidden5/kernel/read\n",
      "hidden5/bias/Initializer/zeros\n",
      "hidden5/bias\n",
      "hidden5/bias/Assign\n",
      "hidden5/bias/read\n",
      "dnn/hidden5/MatMul\n",
      "dnn/hidden5/BiasAdd\n",
      "dnn/hidden5/Relu\n",
      "outputs/kernel/Initializer/random_uniform/shape\n",
      "outputs/kernel/Initializer/random_uniform/min\n",
      "outputs/kernel/Initializer/random_uniform/max\n",
      "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
      "outputs/kernel/Initializer/random_uniform/sub\n",
      "outputs/kernel/Initializer/random_uniform/mul\n",
      "outputs/kernel/Initializer/random_uniform\n",
      "outputs/kernel\n",
      "outputs/kernel/Assign\n",
      "outputs/kernel/read\n",
      "outputs/bias/Initializer/zeros\n",
      "outputs/bias\n",
      "outputs/bias/Assign\n",
      "outputs/bias/read\n",
      "dnn/outputs/MatMul\n",
      "dnn/outputs/BiasAdd\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "loss/Const\n",
      "loss/loss\n",
      "gradients/Shape\n",
      "gradients/grad_ys_0\n",
      "gradients/Fill\n",
      "gradients/loss/loss_grad/Reshape/shape\n",
      "gradients/loss/loss_grad/Reshape\n",
      "gradients/loss/loss_grad/Shape\n",
      "gradients/loss/loss_grad/Tile\n",
      "gradients/loss/loss_grad/Shape_1\n",
      "gradients/loss/loss_grad/Shape_2\n",
      "gradients/loss/loss_grad/Const\n",
      "gradients/loss/loss_grad/Prod\n",
      "gradients/loss/loss_grad/Const_1\n",
      "gradients/loss/loss_grad/Prod_1\n",
      "gradients/loss/loss_grad/Maximum/y\n",
      "gradients/loss/loss_grad/Maximum\n",
      "gradients/loss/loss_grad/floordiv\n",
      "gradients/loss/loss_grad/Cast\n",
      "gradients/loss/loss_grad/truediv\n",
      "gradients/zeros_like\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value/Minimum/y\n",
      "clip_by_value/Minimum\n",
      "clip_by_value/y\n",
      "clip_by_value\n",
      "clip_by_value_1/Minimum/y\n",
      "clip_by_value_1/Minimum\n",
      "clip_by_value_1/y\n",
      "clip_by_value_1\n",
      "clip_by_value_2/Minimum/y\n",
      "clip_by_value_2/Minimum\n",
      "clip_by_value_2/y\n",
      "clip_by_value_2\n",
      "clip_by_value_3/Minimum/y\n",
      "clip_by_value_3/Minimum\n",
      "clip_by_value_3/y\n",
      "clip_by_value_3\n",
      "clip_by_value_4/Minimum/y\n",
      "clip_by_value_4/Minimum\n",
      "clip_by_value_4/y\n",
      "clip_by_value_4\n",
      "clip_by_value_5/Minimum/y\n",
      "clip_by_value_5/Minimum\n",
      "clip_by_value_5/y\n",
      "clip_by_value_5\n",
      "clip_by_value_6/Minimum/y\n",
      "clip_by_value_6/Minimum\n",
      "clip_by_value_6/y\n",
      "clip_by_value_6\n",
      "clip_by_value_7/Minimum/y\n",
      "clip_by_value_7/Minimum\n",
      "clip_by_value_7/y\n",
      "clip_by_value_7\n",
      "clip_by_value_8/Minimum/y\n",
      "clip_by_value_8/Minimum\n",
      "clip_by_value_8/y\n",
      "clip_by_value_8\n",
      "clip_by_value_9/Minimum/y\n",
      "clip_by_value_9/Minimum\n",
      "clip_by_value_9/y\n",
      "clip_by_value_9\n",
      "clip_by_value_10/Minimum/y\n",
      "clip_by_value_10/Minimum\n",
      "clip_by_value_10/y\n",
      "clip_by_value_10\n",
      "clip_by_value_11/Minimum/y\n",
      "clip_by_value_11/Minimum\n",
      "clip_by_value_11/y\n",
      "clip_by_value_11\n",
      "GradientDescent/learning_rate\n",
      "GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/bias/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
      "GradientDescent\n",
      "eval/in_top_k/InTopKV2/k\n",
      "eval/in_top_k/InTopKV2\n",
      "eval/Cast\n",
      "eval/Const\n",
      "eval/accuracy\n",
      "init\n",
      "save/filename/input\n",
      "save/filename\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/Assign_1\n",
      "save/Assign_2\n",
      "save/Assign_3\n",
      "save/Assign_4\n",
      "save/Assign_5\n",
      "save/Assign_6\n",
      "save/Assign_7\n",
      "save/Assign_8\n",
      "save/Assign_9\n",
      "save/Assign_10\n",
      "save/Assign_11\n",
      "save/restore_all\n"
     ]
    }
   ],
   "source": [
    "# 다음으로 훈련해야 할 모든 연산을 가져와야 함(그래프 구조를 모를 때는 모든 연산을 출력해 볼 수 있음)\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 연산을 찾았다면 그래프의 get_operation_by_name()이나 get_tensor_by_name() 메서드를 사용하여 추출할 수 있음\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\") # get_operation_by_name() 사용\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\") # 텐서 y(0 번째 출력)\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\") # get_tensor_by_name() 사용\n",
    "\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 사람들을 위해 중요한 연산들을 모아놓은 컬렉션을 만들기\n",
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.add_to_collection(\"my_important_ops\", op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, accuracy, training_op = tf.get_collection(\"my_important_ops\") # 컬렉션을 통해 쉽고 간단하게 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Yoo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 검증 세트 정확도: 0.9642\n",
      "1 검증 세트 정확도: 0.9628\n",
      "2 검증 세트 정확도: 0.9656\n",
      "3 검증 세트 정확도: 0.9652\n",
      "4 검증 세트 정확도: 0.9642\n",
      "5 검증 세트 정확도: 0.965\n",
      "6 검증 세트 정확도: 0.9686\n",
      "7 검증 세트 정확도: 0.9688\n",
      "8 검증 세트 정확도: 0.9682\n",
      "9 검증 세트 정확도: 0.968\n",
      "10 검증 세트 정확도: 0.9702\n",
      "11 검증 세트 정확도: 0.9714\n",
      "12 검증 세트 정확도: 0.9672\n",
      "13 검증 세트 정확도: 0.97\n",
      "14 검증 세트 정확도: 0.971\n",
      "15 검증 세트 정확도: 0.9722\n",
      "16 검증 세트 정확도: 0.972\n",
      "17 검증 세트 정확도: 0.9712\n",
      "18 검증 세트 정확도: 0.9712\n",
      "19 검증 세트 정확도: 0.9716\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반적으로 모델의 일부분(특히 하위층)만 재사용하는 경우가 많음\n",
    "# import_meta_graph() 함수를 사용해 그래프를 복원하면 원본 그래프를 적재하지만, 관심 대상이 아닌 층은 무시할 수 있음\n",
    "reset_graph()\n",
    "\n",
    "n_hidden4 = 20  # 새 층\n",
    "n_outputs = 10  # 새 층\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "hidden3 = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden3/Relu:0\") # 불러온 3번째 은닉층\n",
    "\n",
    "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"new_hidden4\") # 추가하는 4번째 은닉층\n",
    "new_logits = tf.layers.dense(new_hidden4, n_outputs, name=\"new_outputs\") # 추가하는 최종 출력층\n",
    "\n",
    "with tf.name_scope(\"new_loss\"): # 새로운 비용 함수 설정\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"new_eval\"): # 새로운 평가 설정\n",
    "    correct = tf.nn.in_top_k(new_logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"new_train\"): # 새로운 역전파 학습 설정\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 검증 세트 정확도: 0.919\n",
      "1 검증 세트 정확도: 0.9394\n",
      "2 검증 세트 정확도: 0.9488\n",
      "3 검증 세트 정확도: 0.9526\n",
      "4 검증 세트 정확도: 0.9554\n",
      "5 검증 세트 정확도: 0.9558\n",
      "6 검증 세트 정확도: 0.9576\n",
      "7 검증 세트 정확도: 0.961\n",
      "8 검증 세트 정확도: 0.9612\n",
      "9 검증 세트 정확도: 0.9644\n",
      "10 검증 세트 정확도: 0.9652\n",
      "11 검증 세트 정확도: 0.9662\n",
      "12 검증 세트 정확도: 0.9638\n",
      "13 검증 세트 정확도: 0.967\n",
      "14 검증 세트 정확도: 0.969\n",
      "15 검증 세트 정확도: 0.9682\n",
      "16 검증 세트 정확도: 0.9702\n",
      "17 검증 세트 정확도: 0.9682\n",
      "18 검증 세트 정확도: 0.9694\n",
      "19 검증 세트 정확도: 0.97\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = new_saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.2 다른 프레임워크의 모델 재사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.  83. 105.]]\n"
     ]
    }
   ],
   "source": [
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # 다른 프레임워크로부터 가중치를 로드\n",
    "original_b = [7., 8., 9.]                 # 다른 프레임워크로부터 편향을 로드\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] 모델의 나머지 부분을 구성\n",
    "\n",
    "# hidden1 변수의 할당 노드에 대한 핸들을 구합니다\n",
    "graph = tf.get_default_graph()\n",
    "assign_kernel = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n",
    "assign_bias = graph.get_operation_by_name(\"hidden1/bias/Assign\")\n",
    "init_kernel = assign_kernel.inputs[1]\n",
    "init_bias = assign_bias.inputs[1]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init, feed_dict={init_kernel: original_w, init_bias: original_b})\n",
    "    # [...] 새 작업에 모델을 훈련시킵니다\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))  # 책에는 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.  83. 105.]]\n"
     ]
    }
   ],
   "source": [
    "# 또 다른 방법은 전용 할당 노드와 플레이스홀더를 만든는 것으로 더 번거롭고 효율적이지 않지만 하려는 방식이 잘 드러나는 방법\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3\n",
    "\n",
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # 다른 프레임워크로부터 가중치를 로드\n",
    "original_b = [7., 8., 9.]                 # 다른 프레임워크로부터 편향을 로드\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] 모델의 나머지를 구성\n",
    "\n",
    "# hidden1 변수의 할당 노드에 대한 핸들을 구합니다\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):  # 루트 범위\n",
    "    hidden1_weights = tf.get_variable(\"hidden1/kernel\")\n",
    "    hidden1_biases = tf.get_variable(\"hidden1/bias\")\n",
    "\n",
    "# 전용 플레이스홀더와 할당 노드를 만듭니다\n",
    "original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))\n",
    "original_biases = tf.placeholder(tf.float32, shape=n_hidden1)\n",
    "assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)\n",
    "assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(assign_hidden1_weights, feed_dict={original_weights: original_w})\n",
    "    sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})\n",
    "    # [...] 새 작업에 모델을 훈련시킵니다\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden1/bias:0' shape=(3,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_collection()에 scope를 지정하여 변수의 핸들을 가져올 수도 있음\n",
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 그래프의 get_tensor_by_name() 메서드를 사용할 수도 있음\n",
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/bias:0' shape=(3,) dtype=float32_ref>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/bias:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.3 신경망의 하위층을 학습에서 제외하기\n",
    "- 일반적으로 새로운 DNN을 훈련시킬 때 재사용되는 층들의 가충치를 동결하는 것이 좋음\n",
    "- 하위층의 가중치가 고정되면(학습하려는 대상이 바뀌지 않기 때문에) 상위층의 가중치를 훈련시키기 쉬움\n",
    "    - 훈련하는 동안 하위층을 고정시키는 한 가지 방법은 하위층의 변수를 제외하고 훈련시킬 변수 목록을 옵티마이저에 전달하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # 재사용\n",
    "n_hidden2 = 50  # 재사용\n",
    "n_hidden3 = 50  # 재사용\n",
    "n_hidden4 = 20  # 새로 만듦!\n",
    "n_outputs = 10  # 새로 만듦!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # 재사용\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # 재사용\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # 재사용\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # 새로 만듦!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # 새로 만듦!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "with tf.name_scope(\"train\"): \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                   scope=\"hidden[34]|outputs\") # 은닉층 3, 4와 출력층에 있는 학습할 변수 목록을 모두 구함\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars) # 학습할 변수 목록은 옵티마이저에 제공(은닉층 1, 2는 동결)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 검증 세트 정확도: 0.8962\n",
      "1 검증 세트 정확도: 0.9298\n",
      "2 검증 세트 정확도: 0.9402\n",
      "3 검증 세트 정확도: 0.9438\n",
      "4 검증 세트 정확도: 0.9482\n",
      "5 검증 세트 정확도: 0.9506\n",
      "6 검증 세트 정확도: 0.9506\n",
      "7 검증 세트 정확도: 0.9538\n",
      "8 검증 세트 정확도: 0.9552\n",
      "9 검증 세트 정확도: 0.9564\n",
      "10 검증 세트 정확도: 0.9562\n",
      "11 검증 세트 정확도: 0.9564\n",
      "12 검증 세트 정확도: 0.9574\n",
      "13 검증 세트 정확도: 0.9576\n",
      "14 검증 세트 정확도: 0.959\n",
      "15 검증 세트 정확도: 0.958\n",
      "16 검증 세트 정확도: 0.9574\n",
      "17 검증 세트 정확도: 0.96\n",
      "18 검증 세트 정확도: 0.9594\n",
      "19 검증 세트 정확도: 0.9602\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\") # 정규 표현식\n",
    "restore_saver = tf.train.Saver(reuse_vars) # 1-3층 복원\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.4 동결된 층 캐싱하기\n",
    "- 동결된 층은 변하지 않기 때문에 각 훈련 샘플에 대해 가장 위쪽의 동결된 층에서 나온 출력을 캐싱하는 것이 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # 재사용\n",
    "n_hidden2 = 50  # 재사용\n",
    "n_hidden3 = 50  # 재사용\n",
    "n_hidden4 = 20  # 새로 만듦!\n",
    "n_outputs = 10  # 새로 만듦!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\") # 동결층 재사용\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # 동결층 재사용 & 캐싱\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # 동결하지 않고 재사용\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # 새로 만듦!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # 새로 만듦!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\") # 정규 표현식\n",
    "restore_saver = tf.train.Saver(reuse_vars) # 1-3층 복원\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 검증 세트 정확도: 0.9018\n",
      "1 검증 세트 정확도: 0.931\n",
      "2 검증 세트 정확도: 0.9436\n",
      "3 검증 세트 정확도: 0.9474\n",
      "4 검증 세트 정확도: 0.9514\n",
      "5 검증 세트 정확도: 0.9524\n",
      "6 검증 세트 정확도: 0.9522\n",
      "7 검증 세트 정확도: 0.9552\n",
      "8 검증 세트 정확도: 0.9556\n",
      "9 검증 세트 정확도: 0.9558\n",
      "10 검증 세트 정확도: 0.957\n",
      "11 검증 세트 정확도: 0.9554\n",
      "12 검증 세트 정확도: 0.957\n",
      "13 검증 세트 정확도: 0.9576\n",
      "14 검증 세트 정확도: 0.9576\n",
      "15 검증 세트 정확도: 0.9572\n",
      "16 검증 세트 정확도: 0.9568\n",
      "17 검증 세트 정확도: 0.9576\n",
      "18 검증 세트 정확도: 0.9592\n",
      "19 검증 세트 정확도: 0.9578\n"
     ]
    }
   ],
   "source": [
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    h2_cache = sess.run(hidden2, feed_dict={X: X_train})\n",
    "    h2_cache_valid = sess.run(hidden2, feed_dict={X: X_valid})\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_idx = np.random.permutation(len(X_train))\n",
    "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches) # X가 아닌 h2_cache에서 가져감\n",
    "        y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={hidden2:hidden2_batch, y:y_batch})\n",
    "\n",
    "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_valid, y: y_valid}) # X_valid가 아닌 h2_cache_valid\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.5 상위층을 변경, 삭제, 대체하기\n",
    "- 원본 모델의 출력층괴 상위층은 새로운 모델에서 보통 교체되는데, 새로운 작업에서 재사용할 적절한 층의 개수를 파악해야 함\n",
    "- 이를 위해 복사한 모든 층을 동결하여 성능을 측정한 후, 가장 위쪽의 은닉층 한 두개를 동결 해제하여 역전파로 가중치를 변경한 후 성능을 측정\n",
    "- 이를 반복하여 재사용하기 적절한 층의 개수를 찾음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.6 모델 저장소\n",
    "- 해결하고자 하는 문제와 비슷한 작업을 훈련시킨 신경망을 찾기\n",
    "    - 텐서플로는 https://github.com/tensorflow/models 에 자체 모델 저장소를 가지고 있음\n",
    "    - 카피 모델 저장소 http://goo.gl/XI02X3\n",
    "    - 사우미트로 다스굽타가 만든 카페-텐서플로 변환기 https://gihub.com/ethereon/caffe-tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.7 비지도 사전훈련\n",
    "- 비슷한 작업에 훈련된 모델을 찾을 수 없고 레이블된 훈련 데이터가 많지 않은 복잡한 문제일 경우 비지도 사전훈련을 고려해볼 수 있음\n",
    "- 레이블이 없는 훈련 데이터가 많다면 제한된 볼츠만 머신이나 오토 인코더 같은 특성 추출 알고리즘으로 한 층씩 학습시킬 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2.8 보조 작업으로 사전훈련\n",
    "- 마지막 선택사항은 훈련 데이터를 쉽게 얻거나 생성할 수 있는 보조 작업에 첫 번째 신경망을 훈련시키는 것\n",
    "    - 첫 번째 신경망의 하위층은 두 번째 신경망에 재사용될 수 있는 특성 추출기를 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 고속 옵티마이저\n",
    "- 앞서 살펴 본 훈련 속도를 높이는 네 가지 방법\n",
    "    - 1. 연결 가중치에 좋은 초기화 전략 적용하기\n",
    "    - 2. 좋은 활성화 함수 사용하기\n",
    "    - 3. 배치 정규화 사용하기\n",
    "    - 4. 미리 훈련된 신경망의 일부 재사용하기\n",
    "- 아주 큰 심층 신경망은 훈련이 심각히 느릴 수 있기에 경사 하강법 옵티마이저 대신에 더 빠른 옵티마이저를 사용할 수 있음\n",
    "    - 1. 모멘텀 최적화\n",
    "    - 2. 네스테로프 가속 경사\n",
    "    - 3. AdaGrad\n",
    "    - 4. RMSProp\n",
    "    - 5. Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.1 모멘텀 최적화\n",
    "- 볼링공이 매끈한 표면의 완만한 경사를 굴러내려갈 때 처음에는 느리지만 종단속도에 도달할 때까지는 빠르게 가속되는 원리와 비슷\n",
    "- 모멘텀 최적화가 경사 하강법보다 더 빠르게 평탄한 지역의 탈출하고, 골짜기를 따라 바닥에 도달할 때까지 더 빠르게 내려감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9) # momentum 하이퍼라미터는 보통 0.9로 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.2 네스테로프 가속 경사\n",
    "- 모멘텀 최적화의 한 변종으로, 기본 모멘텀 최적화보다 거의 항상 빠름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_nesterov=True) # use_nesterov를 True로 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.3 AdaGrad\n",
    "- 가파른 차원을 따라 그래디언트 벡터의 스케일을 감소시켜 전역 최적점을 일찍 감지하여 그 쪽으로 방향을 잡을 수 있게 도와줌\n",
    "- 다만, 간단한 문제에는 잘 작동하지만 심층 신경망에서는 너무 빨리 느려져 일찍 멈추는 경향이 있어 심층 신경망에서는 잘 안씀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.4 RMSProp\n",
    "- 너무 일찍 느려지는 AdaGrad의 문제를 가장 최근에 반복에서 비롯된 그래디언트만 누적함으로써 해결한 알고리즘(이를 위해 지수 감소를 사용)\n",
    "- 아주 간단한 문제를 제외하고는 언제나 AdaGrad보다 성능이 좋음(모멘텀 최적화, 네스테로프 가속 경사보다 더 빠르게 수렴)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, momentum=0.9, decay=0.9, epsilon=1e-10) # 감쇠율(decay)는 보통 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.5 Adam 최적화\n",
    "- 적응적 모덴트 추정을 의미하며 이는 모멘텀 최적화와 RMSProp의 아이디어를 합친 것\n",
    "- 모멘텀 최적화처럼 지난 그래디언트 지수 감소 평균을 따르고 RMSProp처럼 지난 그래디언트 제곱의 지수 감소된 평균을 따름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) # learning_rate는 일반적으로 0.001을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 지금까지의 다섯 가지 최적화 기법은 1차 편미분에 의존했으나 최적화 이론에는 2차 편미분을 기반으로 한 뛰어난 알고리즘들이 존재\n",
    "- 하지만 이런 알고리즘은 심층 신경망에 적용하기 어려움(메모리 용량의 부족 문제, 계산 속도의 문제)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3.6 학습률 스케줄링\n",
    "- 학습률이 너무 크면 발산할 수 있고, 너무 작으면 시간이 매우 오래 걸리는 문제가 존재\n",
    "- 높은 학습률로 시작하고 학습 속도가 느려질 때 학습률을 나춘다면 고정 학습률보다 좋은 솔루션에 빨리 도달할 수 있음\n",
    "- 이를 학습 스케줄링이라고 함\n",
    "    - 미리 정의된 개별적인 고정 학습률 - 에포크 마다 학습률을 바꿈\n",
    "    - 성능 기반 스케줄링 - 매 스텝마다 검층 오차를 측정하여 오차가 줄어들지 않으면 학습률을 감소시킴\n",
    "    - 지수 기반 스케줄링 - 반복 횟수를 지수에 넣어 학습률을 설정\n",
    "    - 거듭제곱 기반 스케줄링 - 거듭제곱을 통해 학습률을 설정하는 것으로 지수 기반 스케줄링과 비슷하지만 학습률이 훨씬 느리게 감소함\n",
    "- AdaGrad, RMSProp, Adam 최적화는 훈련하는 동안 자동으로 학습률을 감소시키기 때문에 적용할 필요가 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):       # 학습률 스케줄링 구현\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                               decay_steps, decay_rate) # exponential_decay()를 통한 지수 기반 스케줄링 구현\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 검증 세트 정확도: 0.9656\n",
      "1 검증 세트 정확도: 0.9724\n",
      "2 검증 세트 정확도: 0.9746\n",
      "3 검증 세트 정확도: 0.9812\n",
      "4 검증 세트 정확도: 0.9824\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 과대적합을 피하기 위한 규제 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.1 조기 종료\n",
    "- 훈련 세트에 과대적합되는 것을 피하기 위한 좋은 방법 중 하나로 검증 세트 성능이 떨어지기 시작할 때 훈련을 중지시키면 됨\n",
    "- 텐서플로로 구현하는 한 가지 방법은 다음과 같음\n",
    "    - 1. 일정한 간격(예를 들어 50 스텝마다) 검증 세트로 모델을 평가\n",
    "    - 2. 평가 결과가 이전의 최고 성능보다 나을 경우 이를 최고 성능의 스냅샷으로 저장\n",
    "    - 3. 마지막 스냅샷이 저장된 이후 지난 스텝을 카운트해서 이 숫자가 어떤 한계점을 넘으면 훈련을 중지시킴\n",
    "    - 4. 마지막에 저장한 스냅샷을 복원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.2 $\\ell_1$과 $\\ell_2$ 규제\n",
    "- 4장에서 선형 회귀에 대해 했던 것처럼 $\\ell_1$과 $\\ell_2$ 규제를 사용해 신경망의 연결 가중치에 제약을 가할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반적인 모델 구현 후, 크로스 엔트로피 손실에 l1 손실(즉, 가중치의 절댓값)을 더해 전체 손실을 계산\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")\n",
    "    \n",
    "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "\n",
    "scale = 0.001 # l1 규제 하이퍼파라미터\n",
    "\n",
    "with tf.name_scope(\"loss\"): # l1 손실 구현한 비용 함수 설정\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 검증 세트 정확도: 0.831\n",
      "1 검증 세트 정확도: 0.871\n",
      "2 검증 세트 정확도: 0.8838\n",
      "3 검증 세트 정확도: 0.8934\n",
      "4 검증 세트 정확도: 0.8966\n",
      "5 검증 세트 정확도: 0.8988\n",
      "6 검증 세트 정확도: 0.9016\n",
      "7 검증 세트 정확도: 0.9044\n",
      "8 검증 세트 정확도: 0.9058\n",
      "9 검증 세트 정확도: 0.906\n",
      "10 검증 세트 정확도: 0.9068\n",
      "11 검증 세트 정확도: 0.9054\n",
      "12 검증 세트 정확도: 0.907\n",
      "13 검증 세트 정확도: 0.9084\n",
      "14 검증 세트 정확도: 0.9088\n",
      "15 검증 세트 정확도: 0.9064\n",
      "16 검증 세트 정확도: 0.9066\n",
      "17 검증 세트 정확도: 0.9066\n",
      "18 검증 세트 정확도: 0.9066\n",
      "19 검증 세트 정확도: 0.9052\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 규제를 적용하기 위한 또 다른 방법(tf.layers.dense() 함수에 규제 함수를 전달)\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "\n",
    "scale = 0.001\n",
    "\n",
    "# 동일한 매개변수를 매번 반복하지 않으려고 파이썬의 partial() 함수를 사용(kernel_regularizer 를 사용하여 l1 규제 적용)\n",
    "my_dense_layer = partial(tf.layers.dense, activation=tf.nn.relu, kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n",
    "                            name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) # l1 규제 손실을 계산하기 위한 노드 추가\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\") # 전체 손실에 규제 손실을 추가\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 검증 세트 정확도: 0.8274\n",
      "1 검증 세트 정확도: 0.8766\n",
      "2 검증 세트 정확도: 0.8952\n",
      "3 검증 세트 정확도: 0.9016\n",
      "4 검증 세트 정확도: 0.9078\n",
      "5 검증 세트 정확도: 0.9096\n",
      "6 검증 세트 정확도: 0.9124\n",
      "7 검증 세트 정확도: 0.9154\n",
      "8 검증 세트 정확도: 0.9178\n",
      "9 검증 세트 정확도: 0.919\n",
      "10 검증 세트 정확도: 0.92\n",
      "11 검증 세트 정확도: 0.9224\n",
      "12 검증 세트 정확도: 0.9212\n",
      "13 검증 세트 정확도: 0.9228\n",
      "14 검증 세트 정확도: 0.9222\n",
      "15 검증 세트 정확도: 0.9216\n",
      "16 검증 세트 정확도: 0.9218\n",
      "17 검증 세트 정확도: 0.9228\n",
      "18 검증 세트 정확도: 0.9216\n",
      "19 검증 세트 정확도: 0.9214\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.3 드롭아웃\n",
    "- 심층 신경망에서 가장 인기 있는 규제 방법은 드롭아웃(제외)\n",
    "- 이 알고리즘은 매 훈련 스텝에서 각 뉴런은 임시적으로 드롭아웃될 확률 p를 가짐(이번 스텝에서는 제외되도 다음 스텝에서는 활성화 될 수 있음)\n",
    "    - p는 드롭아웃 비율이라고 하며 보통 50%로 지정\n",
    "    - 훈련이 끝난 후에는 드롭아웃을 적용하지 않음\n",
    "    - p를 50%로 설정하면 한 뉴런이 훈련때보다 두 배 많은 입력 뉴런과 연결되므로 이를 보정하기 위해 0.5를 곱할 필요가 있음\n",
    "    - 이를 보존 확률이라 하며 (1-p)를 곱함\n",
    "- 드롭아웃을 이용하여 신경망을 10,000번의 훈련 스텝을 진행하면 10,000개의 다른 신경망을 훈련시키게 됨\n",
    "- 이 신경망 대부분은 가중치를 공유하고 있어 아주 독립적이지는 않지만 그럼에도 모두 다름\n",
    "- 결과적으로 만들어진 신경망은 이 모든 신경망을 평균한 앙상블로 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-68-86fb0ed660e2>:9: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Yoo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training) # dropout 사용(뉴런 제거, 보존 확률 곱하기 등 수행)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training) # dropout 사용\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 검증 세트 정확도: 0.9264\n",
      "1 검증 세트 정확도: 0.9464\n",
      "2 검증 세트 정확도: 0.9518\n",
      "3 검증 세트 정확도: 0.9554\n",
      "4 검증 세트 정확도: 0.9592\n",
      "5 검증 세트 정확도: 0.963\n",
      "6 검증 세트 정확도: 0.9618\n",
      "7 검증 세트 정확도: 0.965\n",
      "8 검증 세트 정확도: 0.971\n",
      "9 검증 세트 정확도: 0.9686\n",
      "10 검증 세트 정확도: 0.9706\n",
      "11 검증 세트 정확도: 0.9714\n",
      "12 검증 세트 정확도: 0.9692\n",
      "13 검증 세트 정확도: 0.9712\n",
      "14 검증 세트 정확도: 0.9724\n",
      "15 검증 세트 정확도: 0.9704\n",
      "16 검증 세트 정확도: 0.9728\n",
      "17 검증 세트 정확도: 0.973\n",
      "18 검증 세트 정확도: 0.973\n",
      "19 검증 세트 정확도: 0.976\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델이 과대적합되었다고 보여지면 드롭아웃 비율을 올릴 수 있고, 반대의 경우는 드롭아웃 비율을 낮춰야 함\n",
    "- 네트워크에 층이 많을 경우에는 드롭아웃 비율을 높이고 소규모 네트워크에서는 줄이는 것이 좋음\n",
    "- 드롭아웃은 수렴을 상당히 느리게 만들지만 적절히 튜닝하면 매우 좋은 모델을 얻는 경우가 많아짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.4 맥스-노름 규제\n",
    "- 신경망에서 아주 널리 사용되는 또 다른 규제로 각각의 뉴런에 대해 입력의 연결 가중치를 제한함\n",
    "- 텐서플로에는 내장되어 있지 않지만, 구현하기 어렵지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 은닉층의 가중치에 대한 핸들을 얻고 clip_by_norm() 함수를 사용해 가중치를 클리핑하는 연산\n",
    "# 이 후 클리핑된 가중치를 가중치 변수에 할당하는 연산을 만듬\n",
    "threshold = 1.0\n",
    "weights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n",
    "clip_weights = tf.assign(weights, clipped_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 번째 층도 동일하게 수행\n",
    "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n",
    "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
    "clip_weights2 = tf.assign(weights2, clipped_weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기화 연산과 Saver 객체\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 검증 세트 정확도: 0.9566\n",
      "1 검증 세트 정확도: 0.9706\n",
      "2 검증 세트 정확도: 0.9718\n",
      "3 검증 세트 정확도: 0.9768\n",
      "4 검증 세트 정확도: 0.976\n",
      "5 검증 세트 정확도: 0.9776\n",
      "6 검증 세트 정확도: 0.9808\n",
      "7 검증 세트 정확도: 0.9814\n",
      "8 검증 세트 정확도: 0.9792\n",
      "9 검증 세트 정확도: 0.9822\n",
      "10 검증 세트 정확도: 0.9824\n",
      "11 검증 세트 정확도: 0.984\n",
      "12 검증 세트 정확도: 0.9818\n",
      "13 검증 세트 정확도: 0.984\n",
      "14 검증 세트 정확도: 0.9842\n",
      "15 검증 세트 정확도: 0.9838\n",
      "16 검증 세트 정확도: 0.9828\n",
      "17 검증 세트 정확도: 0.9838\n",
      "18 검증 세트 정확도: 0.9832\n",
      "19 검증 세트 정확도: 0.9836\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            clip_weights.eval() # 매 훈련 스텝이 끝난 후 실행\n",
    "            clip_weights2.eval() # 매 훈련 스텝이 끝난 후 실행\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수를 통해 구현하는 방법\n",
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\", collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None # 규제 손실을 위한 항이 없습니다\n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, kernel_regularizer=max_norm_reg, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, kernel_regularizer=max_norm_reg, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 검증 세트 정확도: 0.9558\n",
      "1 검증 세트 정확도: 0.97\n",
      "2 검증 세트 정확도: 0.9732\n",
      "3 검증 세트 정확도: 0.9756\n",
      "4 검증 세트 정확도: 0.9766\n",
      "5 검증 세트 정확도: 0.9782\n",
      "6 검증 세트 정확도: 0.9808\n",
      "7 검증 세트 정확도: 0.9806\n",
      "8 검증 세트 정확도: 0.9814\n",
      "9 검증 세트 정확도: 0.9812\n",
      "10 검증 세트 정확도: 0.9816\n",
      "11 검증 세트 정확도: 0.9816\n",
      "12 검증 세트 정확도: 0.9812\n",
      "13 검증 세트 정확도: 0.9826\n",
      "14 검증 세트 정확도: 0.9818\n",
      "15 검증 세트 정확도: 0.9816\n",
      "16 검증 세트 정확도: 0.9822\n",
      "17 검증 세트 정확도: 0.9828\n",
      "18 검증 세트 정확도: 0.9824\n",
      "19 검증 세트 정확도: 0.983\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "clip_all_weights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"검증 세트 정확도:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4.5 데이터 증식\n",
    "- 기존의 데이터에서 새로운 데이터를 생성해 인공적으로 훈련 세트의 크기를 늘리는 규제 방법(과대적합 방지)\n",
    "- 예를 들어 어떤 사진을 어느정도 이동, 회전하거나 크기를 바꾸거나, 명암 또는 색조를 조절하는 방법을 사용\n",
    "- 이렇게 하면 모델이 위치, 각도, 크기, 명암, 색도 등에 덜 민감해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5 실용적 가이드라인\n",
    "- 아래와 같은 설정은 대부분의 경우 잘 맞을 것\n",
    "    - 초기화          >> He 초기화\n",
    "    - 활성화 함수     >> ELU\n",
    "    - 정규화          >> 배치 정규화\n",
    "    - 규제            >> 드롭아웃\n",
    "    - 옵티마이저      >> 네스테로프 가속 경사\n",
    "    - 학습률 스케줄링 >> 없음\n",
    "- 다음과 같은 경우 위의 기본 설정을 바꿀 필요가 있음\n",
    "    - 좋은 학습률을 찾을 수 없는 경우        >> 지수 감소 같은 학습 스케줄을 추가\n",
    "    - 훈련 세트가 너무 작은 경우             >> 데이터 증식을 수행\n",
    "    - 희소 모델이 필요한 경우                >> l1 규제를 추가\n",
    "    - 더욱 희소한 모델이 필요한 경우         >> 네스테로프 가속 경사나 Adam 옵티마이저 대신 l1 규제와 함께 FTRL 알고리즘 사용\n",
    "    - 실행 속도가 빠른 모델이 필요한 경우    >> 배치 정규화를 빼고 ELU 활성화 함수를 LeakyReLU로 바꿈(희소 모델도 도움이 됨)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.6 연습문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. He 초기화를 사용하여 무작위로 선택한 값이라면 모든 가중치를 같은 값으로 초기화해도 괜찮을까요?\n",
    "- 아니오. 모든 가중치는 독립적으로 샘플링되어야 합니다. 즉, 같은 초깃값을 가지면 안됩니다. 가중치를 무작위로 샘플링하는 중요한 한 가지 목적은 대칭성을 피하기 위함입니다. 모든 가중치가 0이 아니더라도 같은 초기값을 가지면 대칭성이 깨지지 않고(즉, 어떤 층에 있는 모든 뉴런이 동일합니다.) 역전파도 이를 해결할 수 없을 것입니다. 구체적으로 말하면 어떤 층에 있는 모든 뉴런이 항상 같은 가중치를 가지게 됩니다. 이는 층마다 하나의 뉴런이 있는 것과 같으므로 수렴하는데 오랜 시간이 걸립니다. 이런 구성으로 좋은 솔루션에 수렴하는 것은 사실상 불가능합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 편향을 0으로 초기화해도 괜찮을까요?\n",
    "- 편향을 0으로 초기화하는 것은 아무 상관이 없습니다. 또는 편향을 가중치처럼 초기화해도 괜찮습니다. 이들은 큰 차이를 만들지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ReLU보다 ELU 활성화 함수가 나은 세 가지는 무엇인가요?\n",
    "- ReLU에 비해 ELU 함수의 장점은 다음과 같습니다.\n",
    "    - 이 함수는 음수를 받을 수 있어서 뉴런의 평균 출력이 (음수는 절대 출력하지 않는)ReLU 활성화 함수보다 일반적으로 0에 더 가깝습니다. 이는 그래디언트 소실 문제를 완화시켜줍니다.\n",
    "    - 도함수의 값은 항상 0이 아니라서 ReLU 유닛에서 일어나는 죽은 뉴런 현상을 피할 수 있습니다.\n",
    "    - ReLU의 기울기는 z=0일 때 0에서 1로 급격히 바뀌는 반면 이 함수는 어디에서나 도함수가 매끄럽게 바뀝니다. 이런 급격한 변화는 z=0일 때 진동을 발생시키므로 경사 하강법의 속도를 느리게 만듭니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 어떤 경우에 ELU, LeakyReLU(또는 그 변종), ReLU, tanh, logistic, softmax 같은 활성화 함수를 사용해야 하나요?\n",
    "- ELU 함수가 기본값으로 좋습니다. 가능한 한 빠른 신경망을 원한다면 LeakyReLU의 변종을 사용할 수 있습니다. 일반적으로 ELU와 LeakyReLU의 성능이 더 뛰어남에도 ReLU 활성화 함수가 간단하기 때문에 많은 사람들이 선호합니다. 그러나 어떤 경우에는 정확히 0을 출력하는 ReLU 활성화 함수의 기능이 유용할 수 있습니다. 소프르맥스 활성화 함수는 상호 배타적인 클래스에 대한 확률을 출력하는 출력층에 사용됩니다. 하지만 그 외에 은닉층에는 거의 사용되지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. MomentumOptimizer를 사용할 대 momentum 하이퍼파라미터를 너무 1에 가깝게 하면 어떤 일이 일어날까요?\n",
    "- MomentumOptimizer를 사용할 때 momentum 하이퍼파라미터를 1에 가깝에 셋팅하면 알고리즘이 전역 최적점 방향으로 빠르게 진행되겠지만 모멘텀 대문에 최솟값을 지나치게 될 것입니다. 그런 다음 느려져서 되돌아오고, 다시 가속되어 또 지나치게 되는 식입니다. 수렴하기 전에 여러 번 이렇게 진동하게 됩니다. 그러므로 작은 momentum 값을 사용했을 때보다 전반적으로 수렴하는데 훨씬 오랜 시간이 걸릴 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 희소 모델을 만들 수 있는 세가지 방법은 무엇인가요?\n",
    "- 희소 모델(즉, 대부분의 가중치가 0인 모델)을 만드는 첫 번째 방법은 평범하게 모델을 훈련시키고 작은 가중치를 0으로 만드는 것입니다. 두 번째 방법은 (더욱 희소하게 만드려면) 훈련하는 동안 옵티마이저에 희소한 모델을 만들도록 l1 규제를 사용하는 것입니다. 세 번째 방법은 텐서플로의 FTRLOptimizer를 사용하여 쌍대 평균과 l1 규제를 연결하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 드롭아웃이 훈련 속도를 느리게 만드나요? 추론(즉, 새로운 샘플에 대한 예측을 만드는 것)도 느리게 만드나요?\n",
    "- 예. 드롭아웃은 일반적으로 대략 두 배 정도 훈련 속도를 느리게 만듭니다. 그러나 드롭 아웃은 훈련할 때만 적용되므로 추론에는 영향을 미치지 않습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
